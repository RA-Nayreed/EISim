{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cfa0c18",
   "metadata": {},
   "source": [
    "### Notebook for generating heterogeneous server configurations\n",
    "\n",
    "Generate XML server configurations with different server capacity and specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "125963c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import json \n",
    "import shutil \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import numpy as np \n",
    "from lxml import etree \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a1e7a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generated topology and placement data\n",
      "Loaded 247 AP locations\n",
      "\n",
      "Loaded 20 high capacity edge server locations\n",
      "\n",
      "Loaded 100 low capacity edge server locations\n",
      "\n",
      "Loaded 246 topology links\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../AP_locations_1100m_45m.pickle', 'rb') as f: \n",
    "    AP_coords = pickle.load(f)\n",
    "\n",
    "with open('../Server_locations_20_1100m_45m.pickle', 'rb') as f: \n",
    "    s_coords_20 = pickle.load(f)\n",
    "\n",
    "with open('../Server_locations_100_1100m_45m.pickle', 'rb') as f: \n",
    "    s_coords_100 = pickle.load(f)\n",
    "\n",
    "with open('../AP_links_1100m_45m.pickle', 'rb') as f: \n",
    "    links = pickle.load(f)\n",
    "\n",
    "\n",
    "print(\"Loaded generated topology and placement data\")\n",
    "print(f\"Loaded {len(AP_coords)} AP locations\\n\")\n",
    "print(f\"Loaded {len(s_coords_20)} high capacity edge server locations\\n\")\n",
    "print(f\"Loaded {len(s_coords_100)} low capacity edge server locations\\n\")\n",
    "print(f\"Loaded {len(links)} topology links\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f75c90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cluster information for different control topologies\n"
     ]
    }
   ],
   "source": [
    "with open('../server20_cluster_info_centralized.json', 'r', encoding='utf-8') as f:\n",
    "    server20_cluster_info_centralized = json.load(f)\n",
    "\n",
    "with open('../server20_cluster_info_hybrid.json', 'r', encoding='utf-8') as f:\n",
    "    server20_cluster_info_hybrid = json.load(f)\n",
    "\n",
    "with open('../server100_cluster_info_centralized.json', 'r', encoding='utf-8') as f:\n",
    "    server100_cluster_info_centralized = json.load(f)\n",
    "\n",
    "with open('../server100_cluster_info_hybrid.json', 'r', encoding='utf-8') as f:\n",
    "    server100_cluster_info_hybrid = json.load(f)\n",
    "\n",
    "print(\"Loaded cluster information for different control topologies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca1c24",
   "metadata": {},
   "source": [
    "### Define Heterogeneous Server Specifications\n",
    "\n",
    "Three server classes with different capacities and power consumption profiles:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd4a78a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hetereogeneous 20-server scenario total MIPS: 400000\n",
      "Distribution: Class A: 6, Class B: 8, Class C: 6\n",
      "Hetereogeneous 100-server scenario total MIPS: 980000\n",
      "Distribution: Class A: 6, Class B: 8, Class C: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json \n",
    "with open('heter_server_configs.json', 'r') as f: \n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "server_specs_20 = config['server_specs_20']\n",
    "server_specs_100 = config['server_specs_100']\n",
    "\n",
    "total_mips_20 = sum(spec['mips'] * spec['count'] for spec in server_specs_20.values())\n",
    "total_mips_100 = sum(spec['mips'] * spec['count'] for spec in server_specs_100.values())\n",
    "\n",
    "print(f\"Hetereogeneous 20-server scenario total MIPS: {total_mips_20}\")\n",
    "print(f\"Distribution: Class A: {server_specs_20['A']['count']}, Class B: {server_specs_20['B']['count']}, Class C: {server_specs_20['C']['count']}\")\n",
    "print(f\"Hetereogeneous 100-server scenario total MIPS: {total_mips_100}\")\n",
    "print(f\"Distribution: Class A: {server_specs_20['A']['count']}, Class B: {server_specs_20['B']['count']}, Class C: {server_specs_20['C']['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc860aea",
   "metadata": {},
   "source": [
    "### Server class assignment \n",
    "\n",
    "We randomly assign a server class to a server position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12bc2996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20-Server class distribution\n",
      "Counter({'B': 8, 'C': 6, 'A': 6})\n",
      "Assignment: ['C', 'A', 'C', 'A', 'B', 'B', 'C', 'C', 'B', 'B', 'C', 'B', 'A', 'B', 'A', 'C', 'B', 'B', 'A', 'A']\n",
      "\n",
      "100-Server class distribution\n",
      "Counter({'B': 50, 'C': 30, 'A': 20})\n",
      "Assignment: ['A', 'C', 'C', 'C', 'B', 'A', 'C', 'C', 'A', 'C', 'C', 'B', 'B', 'C', 'B', 'B', 'B', 'B', 'C', 'B', 'C', 'A', 'B', 'C', 'B', 'C', 'A', 'A', 'C', 'B', 'B', 'C', 'B', 'C', 'C', 'B', 'B', 'C', 'B', 'C', 'B', 'A', 'B', 'B', 'B', 'C', 'B', 'B', 'C', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'C', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'B', 'B', 'A', 'C', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'C', 'B', 'A', 'C', 'B', 'B', 'B', 'A', 'B', 'C', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "def assign_server_classes(num_servers, spec_dict): \n",
    "    classes = []\n",
    "    for class_name, specs in spec_dict.items():\n",
    "        classes.extend([class_name] * specs['count'])\n",
    "\n",
    "    assert len(classes) == num_servers, f\"Class count mismatch {len(classes)} != {len(num_servers)}\"\n",
    "\n",
    "    random.shuffle(classes)\n",
    "    return classes\n",
    "\n",
    "\n",
    "\n",
    "server_classes_20 = assign_server_classes(20, spec_dict=server_specs_20)\n",
    "server_classes_100 = assign_server_classes(100, spec_dict=server_specs_100)\n",
    "print(\"\\n20-Server class distribution\")\n",
    "print(Counter(server_classes_20))\n",
    "print(f\"Assignment: {server_classes_20}\")\n",
    "print(f\"\\n100-Server class distribution\")\n",
    "print(Counter(server_classes_100))\n",
    "print(f\"Assignment: {server_classes_100}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d6c7b",
   "metadata": {},
   "source": [
    "### Server Specification Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6877df9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created heterogeneous server specification generators\n"
     ]
    }
   ],
   "source": [
    "def server_specification_generator(server_classes, specs_dict):\n",
    "    \"\"\"\n",
    "    Creates a generator that yields server specifications based on assigned classes.\n",
    "    \n",
    "    This generator cycles through servers in order, yielding specs for each based on its class.\n",
    "    \n",
    "    Args:\n",
    "        server_classes: List of class assignments for each server position\n",
    "        specs_dict: Dictionary of specifications for each class\n",
    "        \n",
    "    Returns:\n",
    "        Generator function that yields specifications in the required order\n",
    "    \"\"\"\n",
    "    server_index = 0\n",
    "    \n",
    "    def generator():\n",
    "        nonlocal server_index\n",
    "        \n",
    "        # Get the class for this server\n",
    "        server_class = server_classes[server_index % len(server_classes)]\n",
    "        specs = specs_dict[server_class]\n",
    "        \n",
    "        # Increment index immediately so next call gets next server\n",
    "        # (This is safe because server_class is already retrieved for this call)\n",
    "        server_index += 1\n",
    "        \n",
    "        # Yield specifications in the order expected by the XML generation\n",
    "        yield 'True'  # periphery\n",
    "        yield str(specs['idleConsumption'])  # idleConsumption\n",
    "        yield str(specs['maxConsumption'])  # maxConsumption\n",
    "        \n",
    "        # isOrchestrator - only first server (dc1) in centralized topology\n",
    "        # We need to revert the index for checking if it was 0? \n",
    "        # The original code used 'server_index == 0' BEFORE increment? \n",
    "        # Wait, if we increment first, server_index will be 1 for the first server.\n",
    "        # So we should use (server_index - 1) or check before incrementing.\n",
    "        \n",
    "        current_index = server_index - 1\n",
    "        yield 'True' if current_index == 0 else 'False'\n",
    "        \n",
    "        # Location is handled separately by the XML generation function\n",
    "        \n",
    "        yield str(specs['cores'])  # cores\n",
    "        yield str(specs['mips'])  # mips per core\n",
    "        yield str(specs['ram'])  # ram\n",
    "        yield str(specs['storage'])  # storage\n",
    "        \n",
    "        # Add server class as metadata for analysis\n",
    "        yield server_class  # This will be added as a custom attribute\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# Create generator factories for both scenarios\n",
    "def _20_server_spec_gen():\n",
    "    return server_specification_generator(server_classes_20, server_specs_20)\n",
    "\n",
    "def _100_server_spec_gen():\n",
    "    return server_specification_generator(server_classes_100, server_specs_100)\n",
    "\n",
    "print(\"Created heterogeneous server specification generators\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5340792",
   "metadata": {},
   "source": [
    "### AP Specification Generator\n",
    "\n",
    "Access points -- network routing devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d26e15e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created AP specs generator\n"
     ]
    }
   ],
   "source": [
    "def ap_specs_generator():\n",
    "    yield 'True' # periphery\n",
    "    yield '0' # idleConsumption\n",
    "    yield '1' # maxConsumption\n",
    "    yield 'False' # isOrchestrator\n",
    "    yield '1' # cores\n",
    "    yield '1' # mips\n",
    "    yield '1' # ram\n",
    "    yield '1' # storage\n",
    "    yield 'AP' # class marker for APs\n",
    "\n",
    "print(\"Created AP specs generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d753a",
   "metadata": {},
   "source": [
    "### XML generation for server configurations\n",
    "\n",
    "Functions create the XML structure with different server specifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af6c2297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML generation functions defined\n"
     ]
    }
   ],
   "source": [
    "def create_datacenter_names(AP_locs, server_locs):\n",
    "    \"\"\"\n",
    "    Create unique names for servers and APs.\n",
    "\n",
    "    Server names: dc1, dc2,...,dc{n}\n",
    "    Access point: ap1, ap2,...,ap{n}\n",
    "    \"\"\" \n",
    "    names = {}\n",
    "    for i, loc in enumerate(AP_locs):\n",
    "        names[loc] = f\"ap{i+1}\" \n",
    "    for i, loc in enumerate(server_locs):\n",
    "        names[loc] = f\"dc{i+1}\"\n",
    "    \n",
    "    return names\n",
    "\n",
    "def add_server_specifications(doc, server_locs, dc_names, server_specs_generator_factory, cluster_info):\n",
    "    \"\"\"\n",
    "    Add servers to XML tree \n",
    "\n",
    "    Args: \n",
    "        doc: XML root element\n",
    "        server_locs: list of (x,y) tuples for server locations \n",
    "        dc_names: dict mapping locations to names  \n",
    "        server_specs_generator_factory: factory function for generating server specifications \n",
    "        cluster_info: dict with cluster and cluster head information (or None)\n",
    "    \"\"\" \n",
    "\n",
    "    for i, s_loc in enumerate(server_locs): \n",
    "        gen = server_specs_generator_factory()\n",
    "\n",
    "        dc_name = dc_names[s_loc] \n",
    "        s = etree.SubElement(doc, 'datacenter', name=dc_name)\n",
    "\n",
    "        etree.SubElement(s, 'periphery').text = next(gen)\n",
    "        etree.SubElement(s, 'idleConsumption').text = next(gen)\n",
    "        etree.SubElement(s, 'maxConsumption').text = next(gen)\n",
    "        etree.SubElement(s, 'isOrchestrator').text = next(gen)\n",
    "\n",
    "        # Location \n",
    "        l = etree.SubElement(s, 'location')\n",
    "        etree.SubElement(l, 'x_pos').text = str(s_loc[0])\n",
    "        etree.SubElement(l, 'y_pos').text = str(s_loc[1])\n",
    "\n",
    "        # Server specs\n",
    "        etree.SubElement(s, 'cores').text = next(gen)\n",
    "        etree.SubElement(s, 'mips').text = next(gen)\n",
    "        etree.SubElement(s, 'ram').text = next(gen)\n",
    "        etree.SubElement(s, 'storage').text = next(gen)\n",
    "\n",
    "        # Server class metadata\n",
    "        server_class = next(gen)\n",
    "        etree.SubElement(s, 'serverClass').text = server_class\n",
    "\n",
    "        # Cluster information\n",
    "        if cluster_info is None: \n",
    "            # decentralized, each server is its own cluster\n",
    "            etree.SubElement(s, 'cluster').text = str(i)\n",
    "            etree.SubElement(s, 'clusterHead').text = 'True'\n",
    "        else: \n",
    "            # hybrid or centralized: use provided cluster information\n",
    "            dc = cluster_info[dc_name]\n",
    "            cluster_id = dc['cluster']\n",
    "            is_head = dc['head']\n",
    "            etree.SubElement(s, 'cluster').text = str(cluster_id)\n",
    "            etree.SubElement(s, 'clusterHead').text = str(is_head)\n",
    "    \n",
    "def add_AP_specifcation(doc, AP_locs, dc_names, ap_specs_generator):\n",
    "    \"\"\" \n",
    "    Adds APs to XML tree\n",
    "    \"\"\" \n",
    "    for ap_loc in AP_locs:\n",
    "        gen = ap_specs_generator()\n",
    "        ap_name = dc_names[ap_loc]\n",
    "        ap = etree.SubElement(doc, 'datacenter', name=ap_name)\n",
    "        \n",
    "        etree.SubElement(ap, 'periphery').text = next(gen)\n",
    "        etree.SubElement(ap, 'idleConsumption').text = next(gen)\n",
    "        etree.SubElement(ap, 'maxConsumption').text = next(gen)\n",
    "        etree.SubElement(ap, 'isOrchestrator').text = next(gen)\n",
    "        \n",
    "        l = etree.SubElement(ap, 'location')\n",
    "        etree.SubElement(l, 'x_pos').text = str(ap_loc[0])\n",
    "        etree.SubElement(l, 'y_pos').text = str(ap_loc[1])\n",
    "        \n",
    "        etree.SubElement(ap, 'cores').text = next(gen)\n",
    "        etree.SubElement(ap, 'mips').text = next(gen)\n",
    "        etree.SubElement(ap, 'ram').text = next(gen)\n",
    "        etree.SubElement(ap, 'storage').text = next(gen)\n",
    "\n",
    "def add_links(doc, connections, dc_names):\n",
    "    \"\"\"\n",
    "    Adds network links to document tree\n",
    "    \"\"\" \n",
    "    n = etree.SubElement(doc, 'network_links')\n",
    "    for start_point, end_point in connections: \n",
    "        link = etree.SubElement(n, 'link')\n",
    "        etree.SubElement(link, 'from').text = dc_names[start_point]\n",
    "        etree.SubElement(link, 'to').text = dc_names[end_point]\n",
    "\n",
    "\n",
    "def generate_file(AP_locs, server_locs, connections, server_specs_gen, ap_specs_gen, cluster_info=None):\n",
    "    \"\"\" \n",
    "    Main function to generate a complete XML file\n",
    "    \"\"\" \n",
    "    doc = etree.Element('edge_datacenters')\n",
    "\n",
    "    # Filter out APs that are also in the server location\n",
    "    only_APs = [ap for ap in AP_locs if ap not in server_locs]\n",
    "    dc_names = create_datacenter_names(only_APs, server_locs)\n",
    "\n",
    "    # Add components\n",
    "    add_server_specifications(doc, server_locs, dc_names, server_specs_gen, cluster_info)\n",
    "    add_AP_specifcation(doc, only_APs, dc_names, ap_specs_gen)\n",
    "    add_links(doc, connections, dc_names)\n",
    "\n",
    "    etree.indent(doc, space=\"    \")\n",
    "    return etree.tostring(doc, encoding='UTF-8', xml_declaration=True, pretty_print=True)\n",
    "\n",
    "print(\"XML generation functions defined\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def copy_baseline_files(dest_xml_path, topo, servers):\n",
    "    \"\"\"\n",
    "    Copy baseline configuration files (cloud.xml, applications.xml, edge_devices.xml)\n",
    "    from the homogeneous settings to the heterogeneous settings.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    \n",
    "    dest_dir = os.path.dirname(dest_xml_path)\n",
    "    # Source based on standard naming: settings_{TOPOLOGY}_{SERVERS}servers\n",
    "    # E.g. settings_H_100servers\n",
    "    src_dir_name = f'settings_{topo}_{servers}servers'\n",
    "    # Assuming the notebook is running in Environment_setup/revision/\n",
    "    # and settings are in ../../EISim/EISim_settings/\n",
    "    base_settings_path = '../../EISim/EISim_settings/'\n",
    "    src_dir = os.path.join(base_settings_path, src_dir_name)\n",
    "    \n",
    "    files_to_copy = ['cloud.xml', 'applications.xml', 'edge_devices.xml']\n",
    "    \n",
    "    print(f\"  Copying baseline files from {src_dir}...\")\n",
    "    for filename in files_to_copy:\n",
    "        src = os.path.join(src_dir, filename)\n",
    "        dst = os.path.join(dest_dir, filename)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            # print(f\"    Copied {filename}\")\n",
    "        else:\n",
    "            print(f\"    WARNING: Source file {filename} not found in {src_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9119f",
   "metadata": {},
   "source": [
    "### Generate all configuration files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686369ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate configuration files...\n",
      "Generating hybrid control topology files...\n",
      "Created: ../../EISim/EISim_settings/tmp/settings_H_100servers_hetero/edge_datacenters.xml\n",
      "  Copying baseline files from ../../EISim/EISim_settings/settings_H_100servers...\n",
      "Created: ../../EISim/EISim_settings/tmp/settings_H_20servers_hetero/edge_datacenters.xml\n",
      "  Copying baseline files from ../../EISim/EISim_settings/settings_H_20servers...\n",
      "Generating centralized control topology files...\n",
      "Created: ../../EISim/EISim_settings/tmp/settings_C_100servers_hetero/edge_datacenters.xml\n",
      "  Copying baseline files from ../../EISim/EISim_settings/settings_C_100servers...\n",
      "Created: ../../EISim/EISim_settings/tmp/settings_C_20servers_hetero/edge_datacenters.xml\n",
      "  Copying baseline files from ../../EISim/EISim_settings/settings_C_20servers...\n",
      "Generating decentralized control topology files...\n",
      "Created: ../../EISim/EISim_settings/tmp/settings_D_100servers_hetero/edge_datacenters.xml\n",
      "  Copying baseline files from ../../EISim/EISim_settings/settings_D_100servers...\n",
      "Created: ../../EISim/EISim_settings/tmp/settings_D_20servers_hetero/edge_datacenters.xml\n",
      "  Copying baseline files from ../../EISim/EISim_settings/settings_D_20servers...\n"
     ]
    }
   ],
   "source": [
    "print('Generate configuration files...')\n",
    "\n",
    "print('Generating hybrid control topology files...')\n",
    "\n",
    "# 100 servers hybrid topo config\n",
    "hetero_100_hybrid_file = generate_file(AP_coords, \n",
    "                                    s_coords_100, links, \n",
    "                                    _100_server_spec_gen(), ap_specs_generator,\n",
    "                                    server100_cluster_info_hybrid)\n",
    "\n",
    "\n",
    "filename = '../../EISim/EISim_settings/revision/settings_H_100servers_hetero/edge_datacenters.xml'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as f: \n",
    "    f.write(hetero_100_hybrid_file)\n",
    "print(f\"Created: {filename}\")\n",
    "copy_baseline_files(filename, 'H', 100)\n",
    "\n",
    "# 20 servers hybrid topo config\n",
    "\n",
    "hetero_20_hybrid_file = generate_file(AP_coords, \n",
    "                                    s_coords_20, links, \n",
    "                                    _20_server_spec_gen(), ap_specs_generator,\n",
    "                                    server20_cluster_info_hybrid)\n",
    "\n",
    "\n",
    "filename = '../../EISim/EISim_settings/revision/settings_H_20servers_hetero/edge_datacenters.xml'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as f: \n",
    "    f.write(hetero_20_hybrid_file)\n",
    "print(f\"Created: {filename}\")\n",
    "copy_baseline_files(filename, 'H', 20)\n",
    "\n",
    "print(\"Generating centralized control topology files...\")\n",
    "hetero_100_central_file = generate_file(AP_coords, \n",
    "                                        s_coords_100, \n",
    "                                        links, \n",
    "                                        _100_server_spec_gen(), \n",
    "                                        ap_specs_generator, server100_cluster_info_centralized)\n",
    "filename = '../../EISim/EISim_settings/revision/settings_C_100servers_hetero/edge_datacenters.xml'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as f: \n",
    "    f.write(hetero_100_central_file)\n",
    "print(f\"Created: {filename}\")\n",
    "copy_baseline_files(filename, 'C', 100)\n",
    "\n",
    "\n",
    "hetero_20_central_file = generate_file(AP_coords, \n",
    "                                        s_coords_20, \n",
    "                                        links, \n",
    "                                        _20_server_spec_gen(), \n",
    "                                        ap_specs_generator, server20_cluster_info_centralized)\n",
    "filename = '../../EISim/EISim_settings/revision/settings_C_20servers_hetero/edge_datacenters.xml'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as f: \n",
    "    f.write(hetero_20_central_file)\n",
    "print(f\"Created: {filename}\")\n",
    "copy_baseline_files(filename, 'C', 20)\n",
    "\n",
    "\n",
    "print(\"Generating decentralized control topology files...\")\n",
    "# For decentralized: pass cluster_info=None so each server is its own cluster\n",
    "hetero_100_dec_file = generate_file(AP_coords, \n",
    "                                    s_coords_100, \n",
    "                                    links, \n",
    "                                    _100_server_spec_gen(), \n",
    "                                    ap_specs_generator, \n",
    "                                    None)  # None = decentralized (each server is its own cluster)\n",
    "filename = '../../EISim/EISim_settings/revision/settings_D_100servers_hetero/edge_datacenters.xml'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as f: \n",
    "    f.write(hetero_100_dec_file)\n",
    "print(f\"Created: {filename}\")\n",
    "copy_baseline_files(filename, 'D', 100)\n",
    "\n",
    "\n",
    "hetero_20_dec_file = generate_file(AP_coords, \n",
    "                                    s_coords_20, \n",
    "                                    links, \n",
    "                                    _20_server_spec_gen(), \n",
    "                                    ap_specs_generator, \n",
    "                                    None)  # None = decentralized (each server is its own cluster)\n",
    "filename = '../../EISim/EISim_settings/revision/settings_D_20servers_hetero/edge_datacenters.xml'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as f: \n",
    "    f.write(hetero_20_dec_file)\n",
    "print(f\"Created: {filename}\")\n",
    "copy_baseline_files(filename, 'D', 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066724b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}